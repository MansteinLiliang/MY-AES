{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 1、Test the main_minibatch.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#word size =  5516\n#train_x shape =  (50L, 50L)\ntraining...\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "import time\n",
    "import numpy as np\n",
    "import dataset\n",
    "from model import RNN\n",
    "# use_gpu(-1) # -1:cpu; 0,1,2,..: gpu\n",
    "\n",
    "\n",
    "def define_tensor_size(prompt_id):\n",
    "    tensor_ranges = {\n",
    "        1: (50, 50),\n",
    "        2: (50, 50),\n",
    "        3: (50, 30),\n",
    "        4: (50, 30),\n",
    "        5: (50, 30),\n",
    "        6: (50, 30),\n",
    "        7: (50, 50),\n",
    "        8: (50, 100)\n",
    "    }\n",
    "    return tensor_ranges[prompt_id]\n",
    "e = 0.01\n",
    "lr = 0.1\n",
    "drop_rate = 0.\n",
    "prompt_id = 1\n",
    "vocab_size = 0  # 0 is define to automated infer vocab-size\n",
    "sent_len, doc_len = define_tensor_size(prompt_id)  # sent_len is batch_size of tensor\n",
    "hidden_size = [500]\n",
    "word_embedding_size = 200\n",
    "# try: gru, lstm\n",
    "cell = \"gru\"\n",
    "# try: sgd, momentum, rmsprop, adagrad, adadelta, adam, nesterov_momentum\n",
    "optimizer = \"nesterov_momentum\"\n",
    "train_path, dev_path, test_path = './data/fold_0/train.tsv', './data/fold_0/dev.tsv', './data/fold_0/test.tsv'\n",
    "(train_x, train_masks, train_y), (dev_x, dev_masks, dev_y), (test_x, test_masks, test_y), vocab, vocab_size =\\\n",
    "    dataset.get_data((train_path, dev_path, test_path), prompt_id, vocab_size, doc_len, sent_len)\n",
    "\n",
    "print \"#word size = \", vocab_size\n",
    "print \"#train_x shape = \", train_x[0].shape\n",
    "\n",
    "print \"training...\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2、Test the batch_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 begin......\n#batch shape =  [  8.  10.  10.   9.  10.   9.   9.   8.   8.  10.  10.   8.   6.   8.   7.\n  10.  10.  10.  11.   8.  11.   8.   9.   6.   8.   8.   2.   8.   6.  10.\n   7.   8.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "def batch_generator(X, masks, y, doc_num=1):\n",
    "    epoch = 1\n",
    "    if doc_num==1:\n",
    "        X_new, masks_new, y_new = shuffle(X, masks, y)\n",
    "        print \"epoch: \"+str(epoch)+\" begin......\"\n",
    "        for i in xrange(0,len(X_new),doc_num):\n",
    "            yield X_new[i], masks_new[i], y_new[i]\n",
    "    else:\n",
    "        while(True):\n",
    "            X_new, masks_new, y_new = shuffle(X, masks, y)\n",
    "            print \"epoch: \"+str(epoch)+\" begin......\"\n",
    "            for i in xrange(0,len(X_new),doc_num):\n",
    "                yield np.hstack(X_new[i:i+doc_num]), np.hstack(masks_new[i:i+doc_num]), np.hstack(y_new[i:i+doc_num])\n",
    "            epoch+=1\n",
    "train_batch = batch_generator(\n",
    "    train_x, train_masks, train_y, 32\n",
    ")\n",
    "x,m,y_=train_batch.next()\n",
    "print '#batch shape = ', y_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3、Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compiling...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74.4926834106\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48.9988288879\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32.6952171326\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.2682476044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.5997047424\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.334985733\n"
     ]
    }
   ],
   "source": [
    "print \"compiling...\"\n",
    "from mylayers.encoding_layer import SentEncoderLayer\n",
    "from mylayers.syntax_attention import SyntaxAttentionLayer\n",
    "from mylayers.layer_utils import init_weights, init_bias\n",
    "import theano\n",
    "import numpy as np\n",
    "import theano.tensor as T\n",
    "from optimizers import *\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n",
    "# Word_Embdding layer\n",
    "params=[]\n",
    "rng = RandomStreams(1234)\n",
    "embeddings = theano.shared(0.2 * np.random.uniform(\n",
    "    -1.0, 1.0,(vocab_size, 200)).astype(theano.config.floatX),name='WEmb')  # add one for PADDING at the end\n",
    "params.append(embeddings)\n",
    "idxs = T.imatrix()\n",
    "x=x.astype('int')\n",
    "mask = T.matrix(\"mask\")\n",
    "X = embeddings[idxs].reshape((idxs.shape[0], idxs.shape[1], 200))\n",
    "batch=T.iscalar(\"batch\")\n",
    "sent_encoder_layer = SentEncoderLayer(rng, X, 200, [200],\n",
    "                                      'gru', 'hh', 0,\n",
    "                                      1, doc_len*batch, mask)\n",
    "layer_input = sent_encoder_layer.activation\n",
    "\n",
    "params+=sent_encoder_layer.params\n",
    "# sent_X = layer_input[layer_input.shape[0] - 1, :]\n",
    "sent_X = layer_input[layer_input.shape[0] - 1, :]\n",
    "doc_sent_X = T.reshape(sent_X,(batch,50, sent_encoder_layer.hidden_size))\n",
    "\n",
    "sent_mask = T.switch(mask.sum(axis=0)> 0, 1.0, 0.0)\n",
    "syntax_vector = init_bias(200, 'syntax_vector')\n",
    "\n",
    "syntax_att = SyntaxAttentionLayer('2', (batch, doc_len, sent_encoder_layer.hidden_size),\n",
    "                                  doc_sent_X, sent_mask, syntax_vector)\n",
    "W_a = init_weights([2*200, 200], \"W_a\" + '2')\n",
    "W_u = init_weights([200, 1], \"W_u\"+'2')\n",
    "b = init_bias(200, 'b_a'+'2')\n",
    "#############################################\n",
    "#    syntax attention\n",
    "# syntax_matrix = T.reshape(T.tile(syntax_vector, batch*50), (batch,50,200))\n",
    "# concat = T.concatenate([doc_sent_X, syntax_matrix], axis=2)\n",
    "# strength = T.dot(T.tanh(T.dot(concat, W_a)+b), W_u).flatten()\n",
    "# strength_mask = strength*sent_mask\n",
    "# a = T.nnet.softmax(strength_mask).reshape((batch,50))[:,:,None]\n",
    "# c = (a*doc_sent_X).sum(axis=1)\n",
    "##################################################\n",
    "#   mse\n",
    "c = syntax_att.activation\n",
    "W_out = init_weights((200,1), 'mse_W')\n",
    "b_out = init_bias(1, 'mse_b')\n",
    "params.append(W_out)\n",
    "params.append(b_out)\n",
    "y_pred = T.dot(c, W_out)+b_out\n",
    "\n",
    "y_true = T.vector(('y_true'))\n",
    "res = T.pow(y_pred-y_true.reshape((batch,1)), 2).mean()\n",
    "\n",
    "import theano\n",
    "clip = theano.gradient.grad_clip(res,0.0,1.0)\n",
    "gparams = []\n",
    "\n",
    "for param in params:\n",
    "    gparam = T.grad(clip, param)\n",
    "    gparams.append(gparam)\n",
    "optimizer = eval('sgd')\n",
    "updates = optimizer(params, gparams)\n",
    "\n",
    "f = theano.function([idxs, mask, y_true,batch], \n",
    "                    res,\n",
    "                    on_unused_input='ignore',\n",
    "                    updates=updates,\n",
    "                    allow_input_downcast=True\n",
    "                    )\n",
    "for i in range(6):\n",
    "    print f(x,np.asarray(m, dtype=theano.config.floatX), y_, 32)\n",
    "    \n",
    "# f = theano.function([idxs, mask, y_true], [c,y_pred,res,layer_input[1]],updates=updates, on_unused_input='ignore')\n",
    "# f = theano.function([idxs, mask, y_true], [c,y_pred,res,layer_input[1]],updates=updates, on_unused_input='ignore')\n",
    "# \n",
    "# for i in range(10):\n",
    "#     c_, y_pred_ ,r,layer_input_= f(x,np.asarray(m, dtype=theano.config.floatX), y_)\n",
    "#     # print c_.shape, y_pred_,r,sent_x\n",
    "#     print layer_input_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4、Test the main_minibatch.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "model = RNN(vocab_size, word_embedding_size, hidden_size, cell, 'rmsprop', 0, doc_len)\n",
    "\n",
    "# model.train(x,np.asarray(m, dtype=theano.config.floatX), 0.1, y_, 32)\n",
    "from dataset import dev_test_batch_generator\n",
    "train_batch = dev_test_batch_generator(\n",
    "    train_x, train_masks, train_y, 32)\n",
    "# (x_, mask, y_)\n",
    "count=0\n",
    "for (x_, mask, y_) in train_batch:\n",
    "    if count >20:\n",
    "        break\n",
    "    # model.predict(x_, np.asarray(\n",
    "        # mask, dtype=theano.config.floatX), y_, 32)\n",
    "    print y_.dtype\n",
    "# for i in range(20):\n",
    "#     ex,m,y_=train_batch.next()\n",
    "#     epoch, true_cost, pred=model.train(x,np.asarray(\n",
    "#         m, dtype=theano.config.floatX),\n",
    "#         lr, y_, 1)\n",
    "#     print true_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5、Test creating vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from dataset import create_vocab\n",
    "vocab = create_vocab('./data/fold_0/train.tsv',8,True,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9987"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(0.5, dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import theano.tensor as T\n",
    "import theano\n",
    "import numpy as np\n",
    "\n",
    "from mylayers.layer_utils import init_weights, init_bias\n",
    "b = np.array([[0, 2], [0, 4]]).astype(\"float32\")\n",
    "a = np.array([1]).astype('float32')\n",
    "m= T.matrix('hehe')\n",
    "x= T.tile(m,(1,3))\n",
    "y=T.switch(x.sum(axis=0) > 0, 1.0, 0.0)\n",
    "z=theano.shared(np.array([0,0,0,0,0,0],dtype='float32'))\n",
    "y=T.mean(T.square(z-y))\n",
    "# y=y[None,:]\n",
    "# y[...]=b\n",
    "out = theano.function([m],y)\n",
    "out(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.00509187 -0.06839405 -0.00483718  0.09116028  0.02922797  0.08932855\n   0.16974536  0.26184026  0.16582535  0.30151564]\n [-0.00509187 -0.06839405 -0.00483718  0.09116028  0.02922797  0.08932855\n   0.16974536  0.26184026  0.16582535  0.30151564]]\n(2L, 10L)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x=np.random.randn((10)) * 0.1\n",
    "x=np.tile(x,(1,2))\n",
    "x=x.reshape((2,10))\n",
    "\n",
    "print x\n",
    "print x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100L, 50L)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import pprint\n",
    "import numpy as np\n",
    "from keras.preprocessing import sequence\n",
    "def get_sents(string, vocab, doc_len=100, sent_len=50):\n",
    "    \"\"\"\n",
    "    :param string:\n",
    "    :return: ndarray:shape=(doc_len, sent_len), mask: shape=(doc_len, sent_len), num_hit, unk_hit, total\n",
    "    \"\"\"\n",
    "\n",
    "    num_hit = 0\n",
    "    unk_hit = 0\n",
    "    total = 0\n",
    "    sents = nltk.sent_tokenize(string)\n",
    "    doc_matrix = np.zeros(shape=(doc_len, sent_len))\n",
    "    for i, sent in enumerate(sents):\n",
    "        tokens = nltk.word_tokenize(sent)\n",
    "        indices = []\n",
    "        if i>= doc_len:\n",
    "            break\n",
    "        for word in tokens:\n",
    "            indices.append(1)\n",
    "            total += 1    \n",
    "        doc_matrix[i,:] = sequence.pad_sequences([indices], sent_len, padding='post')\n",
    "    return doc_matrix, num_hit, unk_hit, total\n",
    "x=\"@ORGANIZATION1, @NUM1 out of @NUM2 people surveyed in @LOCATION1, @CAPS1 agree that the use of computers benefits society. I personally concer\" \\\n",
    "             \"n with these @NUM1 people, computers have a positive effect on people.\"\n",
    "# sents = nltk.sent_tokenize(x)\n",
    "print get_sents(x,{'out':1, 'people':2})[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 3, 4], [1, 2, 3]]\n"
     ]
    }
   ],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}