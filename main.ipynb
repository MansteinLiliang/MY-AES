{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 1、Test the main_minibatch.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n\u001b[94m[INFO]\u001b[0m (dataset) Creating vocabulary from: ./data/fold_0/train.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[94m[INFO]\u001b[0m (dataset)   443236 total words, 12133 unique words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[94m[INFO]\u001b[0m (dataset)   4000 vocab size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[94m[INFO]\u001b[0m (dataset)   Vocab size: 4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[94m[INFO]\u001b[0m (dataset) Reading dataset from: ./data/fold_0/train.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[94m[INFO]\u001b[0m (dataset) Removing sequences with more than 50 words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[94m[INFO]\u001b[0m (dataset)   <num> hit rate: 0.00%, <unk> hit rate: 2.19%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[94m[INFO]\u001b[0m (dataset) Reading dataset from: ./data/fold_0/dev.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[94m[INFO]\u001b[0m (dataset) Removing sequences with more than 50 words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[94m[INFO]\u001b[0m (dataset)   <num> hit rate: 0.00%, <unk> hit rate: 2.82%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[94m[INFO]\u001b[0m (dataset) Reading dataset from: ./data/fold_0/test.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[94m[INFO]\u001b[0m (dataset) Removing sequences with more than 50 words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[94m[INFO]\u001b[0m (dataset)   <num> hit rate: 0.00%, <unk> hit rate: 2.84%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#word size =  4000\n#train_x shape =  (50L, 50L)\ntraining...\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "import theano\n",
    "import time\n",
    "import numpy as np\n",
    "import dataset\n",
    "from model import RNN\n",
    "import cPickle as pkl\n",
    "from w2vEmbReader import W2VEmbReader\n",
    "from asap_evaluator import Evaluator\n",
    "from mylayers.layer_utils import floatX\n",
    "# use_gpu(-1) # -1:cpu; 0,1,2,..: gpu\n",
    "\n",
    "\n",
    "def define_tensor_size(prompt_id):\n",
    "    tensor_ranges = {\n",
    "        1: (50, 50),\n",
    "        2: (50, 50),\n",
    "        3: (50, 30),\n",
    "        4: (50, 30),\n",
    "        5: (50, 30),\n",
    "        6: (50, 30),\n",
    "        7: (50, 50),\n",
    "        8: (50, 100)\n",
    "    }\n",
    "    return tensor_ranges[prompt_id]\n",
    "\n",
    "\n",
    "#   configuration\n",
    "e = 0.01\n",
    "lr = 0.001\n",
    "drop_rate = 0.\n",
    "prompt_id = 1\n",
    "vocab_size = 4000  # 0 is define to automated infer vocab-size\n",
    "sent_len, doc_len = define_tensor_size(prompt_id)  # sent_len is batch_size of tensor\n",
    "doc_num = 32  # defining the doc batch_size to accelerate\n",
    "hidden_size = [500]\n",
    "word_embedding_size = 50\n",
    "# try: gru, lstm\n",
    "cell = \"gru\"\n",
    "# try: sgd, momentum, rmsprop, adagrad, adadelta, adam, nesterov_momentum\n",
    "optimizer = \"rmsprop\"\n",
    "train_path, dev_path, test_path = './data/fold_0/train.tsv', './data/fold_0/dev.tsv', './data/fold_0/test.tsv'\n",
    "(train_x, train_masks, train_y_org), (dev_x, dev_masks, dev_y_org), (test_x, test_masks, test_y_org), vocab, vocab_size =\\\n",
    "    dataset.get_data((train_path, dev_path, test_path), prompt_id, vocab_size, doc_len, sent_len)\n",
    "\n",
    "train_y = dataset.get_model_friendly_scores(train_y_org, prompt_id)\n",
    "dev_y = dataset.get_model_friendly_scores(dev_y_org, prompt_id)\n",
    "test_y = dataset.get_model_friendly_scores(test_y_org, prompt_id)\n",
    "\n",
    "assert len(vocab) == vocab_size\n",
    "\n",
    "print \"#word size = \", vocab_size\n",
    "print \"#train_x shape = \", train_x[0].shape\n",
    "U = floatX(np.random.uniform(-0.05, 0.05, size=(vocab_size, word_embedding_size)))\n",
    "U[0] = np.zeros((word_embedding_size,), dtype=theano.config.floatX)\n",
    "evl = Evaluator(dataset, prompt_id, 'None', dev_y_org.astype('int32'), test_y_org.astype('int32'))\n",
    "train_batch = dataset.train_batch_generator(train_x, train_masks, train_y, doc_num)\n",
    "evl_epoch = 0\n",
    "print \"training...\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2、Test the batch_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 begin......\n#batch shape =  [  8.  10.  10.   9.  10.   9.   9.   8.   8.  10.  10.   8.   6.   8.   7.\n  10.  10.  10.  11.   8.  11.   8.   9.   6.   8.   8.   2.   8.   6.  10.\n   7.   8.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "def batch_generator(X, masks, y, doc_num=1):\n",
    "    epoch = 1\n",
    "    if doc_num==1:\n",
    "        X_new, masks_new, y_new = shuffle(X, masks, y)\n",
    "        print \"epoch: \"+str(epoch)+\" begin......\"\n",
    "        for i in xrange(0,len(X_new),doc_num):\n",
    "            yield X_new[i], masks_new[i], y_new[i]\n",
    "    else:\n",
    "        while(True):\n",
    "            X_new, masks_new, y_new = shuffle(X, masks, y)\n",
    "            print \"epoch: \"+str(epoch)+\" begin......\"\n",
    "            for i in xrange(0,len(X_new),doc_num):\n",
    "                yield np.hstack(X_new[i:i+doc_num]), np.hstack(masks_new[i:i+doc_num]), np.hstack(y_new[i:i+doc_num])\n",
    "            epoch+=1\n",
    "train_batch = batch_generator(\n",
    "    train_x, train_masks, train_y, 32\n",
    ")\n",
    "x,m,y_=train_batch.next()\n",
    "print '#batch shape = ', y_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3、Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compiling...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 begin......\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32L, 200L)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32L, 200L)\n"
     ]
    }
   ],
   "source": [
    "print \"compiling...\"\n",
    "from mylayers.encoding_layer import SentEncoderLayer, DocEncoderLayer\n",
    "from mylayers.attention import SyntaxAttentionLayer, MeaningAttention, CoherenceAttention\n",
    "from mylayers.layer_utils import init_weights, init_bias\n",
    "import theano\n",
    "import numpy as np\n",
    "import theano.tensor as T\n",
    "from optimizers import *\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n",
    "# Word_Embdding layer\n",
    "from mylayers import layer_utils\n",
    "params=[]\n",
    "rng = RandomStreams(1234)\n",
    "embeddings = theano.shared(0.2 * np.random.uniform(\n",
    "    -1.0, 1.0,(vocab_size, 200)).astype(theano.config.floatX),name='WEmb')  # add one for PADDING at the end\n",
    "params.append(embeddings)\n",
    "idxs = T.imatrix()\n",
    "\n",
    "mask = T.matrix(\"mask\")\n",
    "X = embeddings[idxs]\n",
    "batch=T.iscalar(\"batch\")\n",
    "sent_encoder_layer = SentEncoderLayer(rng, X, 200, [200],\n",
    "                                      'gru', 'hh', 0,\n",
    "                                      1, doc_len*batch, mask)\n",
    "layer_input = sent_encoder_layer.activation\n",
    "\n",
    "params+=sent_encoder_layer.params\n",
    "# sent_X = layer_input[layer_input.shape[0] - 1, :]\n",
    "sent_X = layer_input[layer_input.shape[0] - 1, :]\n",
    "doc_sent_X = T.reshape(sent_X, (batch, 50, sent_encoder_layer.hidden_size))\n",
    "\n",
    "sent_mask = T.switch(mask.sum(axis=0) > 0, 1.0, 0.0)\n",
    "###################################################################################\n",
    "# meaning coherence\n",
    "sent_mask = T.transpose(sent_mask.reshape((32, -1)))\n",
    "X_T = doc_sent_X.dimshuffle([1, 0, 2])\n",
    "# Now the total_sents is the total_docs\n",
    "doc_encoder_layer = DocEncoderLayer(rng, X_T, 200, 200, 'gru', optimizer, 0, 1, 32, sent_mask)\n",
    "meaning_h = doc_encoder_layer.activation\n",
    "meaning_att = MeaningAttention((50, 32, 200), X_T, meaning_h, sent_mask).activation\n",
    "\n",
    "####################################################################################\n",
    "# syntax_vector = init_bias(200, 'syntax_vector')\n",
    "# \n",
    "# syntax_att = SyntaxAttentionLayer('2', (batch, doc_len, sent_encoder_layer.hidden_size),\n",
    "#                                   doc_sent_X, sent_mask, syntax_vector)\n",
    "############################################################################################\n",
    "# coherence att\n",
    "# X_T = doc_sent_X.dimshuffle([1, 0, 2])\n",
    "# sent_mask = sent_mask.reshape((doc_num, 50))\n",
    "# M_T = T.transpose(sent_mask)\n",
    "# padding_vector = init_bias(200, \"sent_padding\", 0.0)\n",
    "# X_T_PAD = T.concatenate([\n",
    "#     T.tile(padding_vector, (1, doc_num)).reshape((1, doc_num, -1)),\n",
    "#     X_T,\n",
    "#     T.tile(padding_vector, (1, doc_num)).reshape((1, doc_num, -1))\n",
    "# ])\n",
    "# coherence_vector = init_weights((200, ),name='coherence')\n",
    "# W_a = init_weights((200*3, 200), 'coherence' + \"_W_a\")\n",
    "# b_a = init_bias(200, \"coherence\" + \"_b_a\")\n",
    "# def _active_mask(x_pre, x, x_next, coherence_vector):\n",
    "#     #  x is of shape(doc_nums, rnn_dim)\n",
    "#     #\n",
    "#     x=x.reshape((32, 200))\n",
    "#     x_pre=x_pre.reshape((32, 200))\n",
    "#     x_next=x_next.reshape((32, 200))\n",
    "#     concat = T.concatenate([x, x_pre, x_next], axis=1)\n",
    "#     h_hat = T.tanh(T.dot(concat, W_a)+b_a)\n",
    "#     strength = T.dot(h_hat, coherence_vector)\n",
    "#     return strength\n",
    "# h, updates = theano.scan(_active_mask, sequences=[dict(input=X_T_PAD, taps=[-1, 0, 1])],\n",
    "#                          non_sequences=coherence_vector)\n",
    "# strength_mask = h*M_T\n",
    "# strength_mask_T = T.transpose(strength_mask)\n",
    "# # # a is of shape(num_docs, num_sents)\n",
    "# a = T.nnet.softmax(strength_mask_T)\n",
    "# c = (a[:, :, None]*doc_sent_X).sum(axis=1)\n",
    "###################################################################################################\n",
    "#    syntax attention\n",
    "# W_a = init_weights([2*200, 200], \"W_a\" + '2')\n",
    "# W_u = init_weights([200, 1], \"W_u\"+'2')\n",
    "# b = init_bias(200, 'b_a'+'2')\n",
    "# syntax_matrix = T.reshape(T.tile(syntax_vector, batch*50), (batch,50,200))\n",
    "# concat = T.concatenate([doc_sent_X, syntax_matrix], axis=2)\n",
    "# strength = T.dot(T.tanh(T.dot(concat, W_a)+b), W_u).flatten()\n",
    "# strength_mask = strength*sent_mask\n",
    "# a = T.nnet.softmax(strength_mask).reshape((batch,50))[:,:,None]\n",
    "# c = (a*doc_sent_X).sum(axis=1)\n",
    "########################################################################################################\n",
    "#   mse\n",
    "# c = syntax_att.activation\n",
    "# W_out = init_weights((200,1), 'mse_W')\n",
    "# b_out = init_bias(1, 'mse_b')\n",
    "# params.append(W_out)\n",
    "# params.append(b_out)\n",
    "# y_pred = T.dot(c, W_out)+b_out\n",
    "# \n",
    "# y_true = T.vector(('y_true'))\n",
    "# res = T.square(y_pred.flatten()-y_true).mean()\n",
    "\n",
    "# import theano\n",
    "# clip = theano.gradient.grad_clip(res, 0.0, 1.0)\n",
    "# gparams = []\n",
    "# \n",
    "# for param in params:\n",
    "#     gparam = T.grad(clip, param)\n",
    "#     gparams.append(gparam)\n",
    "# optimizer = eval('sgd')\n",
    "# updates = optimizer(params, gparams)\n",
    "\n",
    "f = theano.function([idxs, mask, batch], \n",
    "                    meaning_att,\n",
    "                    on_unused_input='ignore',\n",
    "                    # updates=updates,\n",
    "                    allow_input_downcast=True,\n",
    "                    )\n",
    "for i in range(2):\n",
    "    epoch, X, mask, y = train_batch.next()\n",
    "    print f(X,np.asarray(mask, dtype=theano.config.floatX), 32).shape\n",
    "    \n",
    "# f = theano.function([idxs, mask, y_true], [c,y_pred,res,layer_input[1]],updates=updates, on_unused_input='ignore')\n",
    "# f = theano.function([idxs, mask, y_true], [c,y_pred,res,layer_input[1]],updates=updates, on_unused_input='ignore')\n",
    "# \n",
    "# for i in range(10):\n",
    "#     c_, y_pred_ ,r,layer_input_= f(x,np.asarray(m, dtype=theano.config.floatX), y_)\n",
    "#     # print c_.shape, y_pred_,r,sent_x\n",
    "#     print layer_input_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 0. ,  0.5,  1. ],\n       [ 0. ,  0.5,  1. ],\n       [ 0. ,  0.5,  1. ],\n       [ 0. ,  0.5,  1. ],\n       [ 0. ,  0.5,  1. ]]), array([[ 0.  ,  0.  ,  0.  ],\n       [ 0.25,  0.25,  0.25],\n       [ 0.5 ,  0.5 ,  0.5 ],\n       [ 0.75,  0.75,  0.75],\n       [ 1.  ,  1.  ,  1.  ]])]\n"
     ]
    }
   ],
   "source": [
    "x = np.linspace(0, 1, 3)\n",
    "y = np.linspace(0, 1, 5)\n",
    "print np.meshgrid(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4、Test the main_minibatch.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "model = RNN(vocab_size, word_embedding_size, hidden_size, cell, 'rmsprop', 0, doc_len)\n",
    "\n",
    "# model.train(x,np.asarray(m, dtype=theano.config.floatX), 0.1, y_, 32)\n",
    "from dataset import dev_test_batch_generator\n",
    "train_batch = dev_test_batch_generator(\n",
    "    train_x, train_masks, train_y, 32)\n",
    "# (x_, mask, y_)\n",
    "count=0\n",
    "for (x_, mask, y_) in train_batch:\n",
    "    if count >20:\n",
    "        break\n",
    "    # model.predict(x_, np.asarray(\n",
    "        # mask, dtype=theano.config.floatX), y_, 32)\n",
    "    print y_.dtype\n",
    "# for i in range(20):\n",
    "#     ex,m,y_=train_batch.next()\n",
    "#     epoch, true_cost, pred=model.train(x,np.asarray(\n",
    "#         m, dtype=theano.config.floatX),\n",
    "#         lr, y_, 1)\n",
    "#     print true_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5、Test creating vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from dataset import create_vocab\n",
    "vocab = create_vocab('./data/fold_0/train.tsv',8,True,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9987"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6、Test Theano Basic Op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "('You cannot drop a non-broadcastable dimension.', ((False, False), (0, 'x')))",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mValueError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-906837e26d32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshared\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[1;31m# y = x.reshape((6,2))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[1;31m# y=T.switch(x.sum(axis=0) > 0, 1.0, 0.0)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mE:\\Anaconda2\\lib\\site-packages\\theano\\tensor\\var.pyc\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    523\u001b[0m                         \u001b[0mcounter\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m                         \u001b[0mnew_args\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 525\u001b[0;31m                 \u001b[0mview\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdimshuffle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    526\u001b[0m                 \u001b[0mrval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mview\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mrval\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mE:\\Anaconda2\\lib\\site-packages\\theano\\tensor\\var.pyc\u001b[0m in \u001b[0;36mdimshuffle\u001b[0;34m(self, *pattern)\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mpattern\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpattern\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m         op = theano.tensor.basic.DimShuffle(list(self.type.broadcastable),\n\u001b[0;32m--> 361\u001b[0;31m                                             pattern)\n\u001b[0m\u001b[1;32m    362\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mE:\\Anaconda2\\lib\\site-packages\\theano\\tensor\\elemwise.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_broadcastable, new_order, inplace)\u001b[0m\n\u001b[1;32m    163\u001b[0m                     raise ValueError(\n\u001b[1;32m    164\u001b[0m                         \u001b[1;34m\"You cannot drop a non-broadcastable dimension.\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m                         (input_broadcastable, new_order))\n\u001b[0m\u001b[1;32m    166\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[1;31m# this is the list of the original dimensions that we keep\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: ('You cannot drop a non-broadcastable dimension.', ((False, False), (0, 'x')))"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "import theano.tensor as T\n",
    "import theano\n",
    "import numpy as np\n",
    "\n",
    "from mylayers.layer_utils import init_weights, init_bias\n",
    "b = np.array([[0, 2], [0, 4]]).astype(\"float32\")\n",
    "a = np.array([1]).astype('float32')\n",
    "m= T.matrix('hehe')\n",
    "t = theano.shared(np.ones((6,))).reshape((1,6))\n",
    "x= T.tile(m,(1,3))\n",
    "x = T.concatenate([t, x],axis=0)\n",
    "# y = x.reshape((6,2))\n",
    "# y=T.switch(x.sum(axis=0) > 0, 1.0, 0.0)\n",
    "# z=theano.shared(np.array([0,0,0,0,0,0],dtype='float32'))\n",
    "# y=T.mean(T.square(z-y))\n",
    "# y=y[None,:]\n",
    "# y[...]=b\n",
    "out = theano.function([m],x)\n",
    "print out(b).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.00509187 -0.06839405 -0.00483718  0.09116028  0.02922797  0.08932855\n   0.16974536  0.26184026  0.16582535  0.30151564]\n [-0.00509187 -0.06839405 -0.00483718  0.09116028  0.02922797  0.08932855\n   0.16974536  0.26184026  0.16582535  0.30151564]]\n(2L, 10L)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x=np.random.randn((10)) * 0.1\n",
    "x=np.tile(x,(1,2))\n",
    "x=x.reshape((2,10))\n",
    "\n",
    "print x\n",
    "print x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100L, 50L)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import pprint\n",
    "import numpy as np\n",
    "from keras.preprocessing import sequence\n",
    "def get_sents(string, vocab, doc_len=100, sent_len=50):\n",
    "    \"\"\"\n",
    "    :param string:\n",
    "    :return: ndarray:shape=(doc_len, sent_len), mask: shape=(doc_len, sent_len), num_hit, unk_hit, total\n",
    "    \"\"\"\n",
    "\n",
    "    num_hit = 0\n",
    "    unk_hit = 0\n",
    "    total = 0\n",
    "    sents = nltk.sent_tokenize(string)\n",
    "    doc_matrix = np.zeros(shape=(doc_len, sent_len))\n",
    "    for i, sent in enumerate(sents):\n",
    "        tokens = nltk.word_tokenize(sent)\n",
    "        indices = []\n",
    "        if i>= doc_len:\n",
    "            break\n",
    "        for word in tokens:\n",
    "            indices.append(1)\n",
    "            total += 1    \n",
    "        doc_matrix[i,:] = sequence.pad_sequences([indices], sent_len, padding='post')\n",
    "    return doc_matrix, num_hit, unk_hit, total\n",
    "x=\"@ORGANIZATION1, @NUM1 out of @NUM2 people surveyed in @LOCATION1, @CAPS1 agree that the use of computers benefits society. I personally concer\" \\\n",
    "             \"n with these @NUM1 people, computers have a positive effect on people.\"\n",
    "# sents = nltk.sent_tokenize(x)\n",
    "print get_sents(x,{'out':1, 'people':2})[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 3, 4], [1, 2, 3]]\n"
     ]
    }
   ],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}