{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Test the main_minibatch.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#word size =  5516\n#train_x shape =  (50L, 50L)\ntraining...\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "import time\n",
    "import numpy as np\n",
    "import dataset\n",
    "from model import RNN\n",
    "# use_gpu(-1) # -1:cpu; 0,1,2,..: gpu\n",
    "\n",
    "\n",
    "def define_tensor_size(prompt_id):\n",
    "    tensor_ranges = {\n",
    "        1: (50, 50),\n",
    "        2: (50, 50),\n",
    "        3: (50, 30),\n",
    "        4: (50, 30),\n",
    "        5: (50, 30),\n",
    "        6: (50, 30),\n",
    "        7: (50, 50),\n",
    "        8: (50, 100)\n",
    "    }\n",
    "    return tensor_ranges[prompt_id]\n",
    "e = 0.01\n",
    "lr = 0.1\n",
    "drop_rate = 0.\n",
    "prompt_id = 1\n",
    "vocab_size = 0  # 0 is define to automated infer vocab-size\n",
    "sent_len, doc_len = define_tensor_size(prompt_id)  # sent_len is batch_size of tensor\n",
    "hidden_size = [500]\n",
    "word_embedding_size = 200\n",
    "# try: gru, lstm\n",
    "cell = \"gru\"\n",
    "# try: sgd, momentum, rmsprop, adagrad, adadelta, adam, nesterov_momentum\n",
    "optimizer = \"nesterov_momentum\"\n",
    "train_path, dev_path, test_path = './data/fold_0/train.tsv', './data/fold_0/dev.tsv', './data/fold_0/test.tsv'\n",
    "(train_x, train_masks, train_y), (dev_x, dev_masks, dev_y), (test_x, test_masks, test_y), vocab, vocab_size =\\\n",
    "    dataset.get_data((train_path, dev_path, test_path), prompt_id, vocab_size, doc_len, sent_len)\n",
    "\n",
    "print \"#word size = \", vocab_size\n",
    "print \"#train_x shape = \", train_x[0].shape\n",
    "\n",
    "print \"training...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#batch shape =  (50L, 50L)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "def batch_generator(X, masks, y):\n",
    "    while(True):\n",
    "        for X_, mask,y_ in zip(*shuffle(X, masks, y)):\n",
    "            yield (X_, mask, y_)\n",
    "train_batch = batch_generator(train_x, train_masks, train_y)\n",
    "x,m,y_=train_batch.next()\n",
    "print '#batch shape = ', x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compiling...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1L, 200L) [[-0.00576363]] 36.0691947937 36.0691947937\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1L, 200L) [[ 4.12832594]] 3.50316381454 3.50316381454\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1L, 200L) [[ 12.34467888]] 40.2549514771 40.2549514771\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1L, 200L) [[ 2.01836085]] 15.8534498215 15.8534498215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1L, 200L) [[ 2.99306202]] 9.04167556763 9.04167556763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1L, 200L) [[ 27.61671638]] 467.282440186 467.282440186\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1L, 200L) [[-13.13246346]] 366.051147461 366.051147461\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1L, 200L) [[ 3.25847411]] 7.51596403122 7.51596403122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1L, 200L) [[ 4.81119967]] 1.41324627399 1.41324627399\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1L, 200L) [[ 7.05557489]] 1.11423838139 1.11423838139\n"
     ]
    }
   ],
   "source": [
    "print \"compiling...\"\n",
    "from mylayers.encoding_layer import SentEncoderLayer\n",
    "from mylayers.syntax_attention import SyntaxAttentionLayer\n",
    "from mylayers.utils import init_weights, init_bias\n",
    "import theano\n",
    "import numpy as np\n",
    "import theano.tensor as T\n",
    "from optimizers import *\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n",
    "# Word_Embdding layer\n",
    "params=[]\n",
    "rng = RandomStreams(1234)\n",
    "embeddings = theano.shared(0.2 * np.random.uniform(\n",
    "    -1.0, 1.0,(vocab_size, 200)).astype(theano.config.floatX),name='WEmb')  # add one for PADDING at the end\n",
    "params.append(embeddings)\n",
    "idxs = T.imatrix()\n",
    "x=x.astype('int')\n",
    "mask = T.matrix(\"mask\")\n",
    "X = embeddings[idxs].reshape((idxs.shape[0], idxs.shape[1], 200))\n",
    "sent_encoder_layer = SentEncoderLayer(rng, X, 200, [200],\n",
    "                                      'gru', 'hh', 0,\n",
    "                                      1, doc_len, mask)\n",
    "layer_input = sent_encoder_layer.activation\n",
    "\n",
    "params+=sent_encoder_layer.params\n",
    "# sent_X = layer_input[layer_input.shape[0] - 1, :]\n",
    "sent_X = layer_input[layer_input.shape[0] - 1, :]\n",
    "\n",
    "sent_mask = T.switch(mask.sum(axis=0)> 0, 1.0, 0.0)\n",
    "syntax_vector = init_bias(200, 'syntax_vector')\n",
    "\n",
    "# W_a = init_weights([2*200, 200],name='W_a')\n",
    "# W_u = init_weights([200, 1],name='W_u')\n",
    "# b = init_bias(200,name='b')\n",
    "# params.extend([W_a, W_u, b])\n",
    "# syntax_matrix = T.reshape(T.tile(syntax_vector, 50), (50,200))\n",
    "# concat = T.concatenate([sent_X, syntax_matrix], axis=1)\n",
    "# strength = T.dot(T.tanh(T.dot(concat, W_a)+b), W_u).flatten()\n",
    "# strength_mask = strength*sent_mask\n",
    "# a = T.nnet.softmax(strength_mask)\n",
    "# c = T.dot(a, sent_X)\n",
    "syntax_att = SyntaxAttentionLayer('2', (doc_len, sent_encoder_layer.hidden_size),\n",
    "                                  sent_X, sent_mask, syntax_vector)\n",
    "c = syntax_att.activation\n",
    "W_out = init_weights((200,1), 'mse_W')\n",
    "b_out = init_bias(1, 'mse_b')\n",
    "params.append(W_out)\n",
    "params.append(b_out)\n",
    "y_pred = T.dot(c, W_out)+b_out\n",
    "y_true = T.scalar(('y_true'))\n",
    "res = T.pow(y_pred-y_true, 2).mean()\n",
    "# clip = T.clip(res, 0.0, 5)\n",
    "import theano\n",
    "clip = theano.gradient.grad_clip(res,0.0,1.0)\n",
    "gparams = []\n",
    "\n",
    "for param in params:\n",
    "    gparam = T.grad(clip, param)\n",
    "    gparams.append(gparam)\n",
    "optimizer = eval('sgd')\n",
    "updates = optimizer(params, gparams)\n",
    "\n",
    "f = theano.function([idxs, mask, y_true], [c,y_pred,res,clip],updates=updates, on_unused_input='ignore')\n",
    "for i in range(10):\n",
    "    c_, y_pred_ ,r,clip_= f(x,np.asarray(m, dtype=theano.config.floatX), y_)\n",
    "    print c_.shape, y_pred_,r,clip_\n",
    "    # print params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50L, 50L) (50L, 50L)\n[[  41.   44.  216. ...,    0.    0.    0.]\n [  10.   17.  109. ...,    0.    0.    0.]\n [  23.   24.  508. ...,    0.    0.    0.]\n ..., \n [   0.    0.    0. ...,    0.    0.    0.]\n [   0.    0.    0. ...,    0.    0.    0.]\n [   0.    0.    0. ...,    0.    0.    0.]]\n8.0\n(50L, 50L) (50L, 50L)\n[[   41.    10.    12. ...,     0.     0.     0.]\n [  110.    18.   385. ...,     0.     0.     0.]\n [    4.  1263.   451. ...,     0.     0.     0.]\n ..., \n [    0.     0.     0. ...,     0.     0.     0.]\n [    0.     0.     0. ...,     0.     0.     0.]\n [    0.     0.     0. ...,     0.     0.     0.]]\n10.0\n(50L, 50L) (50L, 50L)\n[[  41.   19.   19. ...,    0.    0.    0.]\n [ 110.   27.   59. ...,    0.    0.    0.]\n [   1.   33.   10. ...,    0.    0.    0.]\n ..., \n [   0.    0.    0. ...,    0.    0.    0.]\n [   0.    0.    0. ...,    0.    0.    0.]\n [   0.    0.    0. ...,    0.    0.    0.]]\n8.0\n(50L, 50L) (50L, 50L)\n[[  41.   19.   10. ...,    0.    0.    0.]\n [ 110.   55.   58. ...,    0.    0.    0.]\n [ 207.   22.  209. ...,    0.    0.    0.]\n ..., \n [   0.    0.    0. ...,    0.    0.    0.]\n [   0.    0.    0. ...,    0.    0.    0.]\n [   0.    0.    0. ...,    0.    0.    0.]]\n9.0\n(50L, 50L) (50L, 50L)\n[[  41.   52.    6. ...,    0.    0.    0.]\n [ 564.   18.   98. ...,    0.    0.    0.]\n [ 132.   60.  103. ...,    0.    0.    0.]\n ..., \n [   0.    0.    0. ...,    0.    0.    0.]\n [   0.    0.    0. ...,    0.    0.    0.]\n [   0.    0.    0. ...,    0.    0.    0.]]\n7.0\n(50L, 50L) (50L, 50L)\n[[  41.  429.   81. ...,    0.    0.    0.]\n [ 110.    4.   14. ...,    0.    0.    0.]\n [ 160.   19.  114. ...,    0.    0.    0.]\n ..., \n [   0.    0.    0. ...,    0.    0.    0.]\n [   0.    0.    0. ...,    0.    0.    0.]\n [   0.    0.    0. ...,    0.    0.    0.]]\n10.0\n(50L, 50L) (50L, 50L)\n[[  41.  429.   23. ...,    0.    0.    0.]\n [ 110.    4.  236. ...,    0.    0.    0.]\n [   1.   19.   46. ...,    0.    0.    0.]\n ..., \n [   0.    0.    0. ...,    0.    0.    0.]\n [   0.    0.    0. ...,    0.    0.    0.]\n [   0.    0.    0. ...,    0.    0.    0.]]\n11.0\n(50L, 50L) (50L, 50L)\n[[  41.   19.  291. ...,    0.    0.    0.]\n [  20.   59.    4. ...,    0.    0.    0.]\n [  40.   35.   24. ...,    0.    0.    0.]\n ..., \n [   0.    0.    0. ...,    0.    0.    0.]\n [   0.    0.    0. ...,    0.    0.    0.]\n [   0.    0.    0. ...,    0.    0.    0.]]\n10.0\n(50L, 50L) (50L, 50L)\n[[  41.   40.   40. ...,    0.    0.    0.]\n [ 110.  170.  291. ...,    0.    0.    0.]\n [   4.  102.  102. ...,    0.    0.    0.]\n ..., \n [   0.    0.    0. ...,    0.    0.    0.]\n [   0.    0.    0. ...,    0.    0.    0.]\n [   0.    0.    0. ...,    0.    0.    0.]]\n7.0\n(50L, 50L) (50L, 50L)\n[[  4.10000000e+01   5.00000000e+00   3.49300000e+03 ...,   0.00000000e+00\n    0.00000000e+00   0.00000000e+00]\n [  1.10000000e+02   6.00000000e+00   1.00000000e+00 ...,   0.00000000e+00\n    0.00000000e+00   0.00000000e+00]\n [  1.00000000e+00   6.04000000e+02   1.00000000e+00 ...,   0.00000000e+00\n    0.00000000e+00   0.00000000e+00]\n ..., \n [  0.00000000e+00   0.00000000e+00   0.00000000e+00 ...,   0.00000000e+00\n    0.00000000e+00   0.00000000e+00]\n [  0.00000000e+00   0.00000000e+00   0.00000000e+00 ...,   0.00000000e+00\n    0.00000000e+00   0.00000000e+00]\n [  0.00000000e+00   0.00000000e+00   0.00000000e+00 ...,   0.00000000e+00\n    0.00000000e+00   0.00000000e+00]]\n10.0\n(50L, 50L) (50L, 50L)\n[[  41.   19.  291. ...,    0.    0.    0.]\n [ 110.   59.  444. ...,    0.    0.    0.]\n [ 207.   35.   10. ...,    0.    0.    0.]\n ..., \n [   0.    0.    0. ...,    0.    0.    0.]\n [   0.    0.    0. ...,    0.    0.    0.]\n [   0.    0.    0. ...,    0.    0.    0.]]\n8.0\n(50L, 50L) (50L, 50L)\n[[  41.   40.  204. ...,    0.    0.    0.]\n [ 110.  102.    8. ...,    0.    0.    0.]\n [ 207.   17.   59. ...,    0.    0.    0.]\n ..., \n [   0.    0.    0. ...,    0.    0.    0.]\n [   0.    0.    0. ...,    0.    0.    0.]\n [   0.    0.    0. ...,    0.    0.    0.]]\n8.0\n(50L, 50L) (50L, 50L)\n[[   19.    34.    10. ...,     0.     0.     0.]\n [   59.    64.    58. ...,     0.     0.     0.]\n [   13.    26.  1907. ...,     0.     0.     0.]\n ..., \n [    7.     0.     0. ...,     0.     0.     0.]\n [   47.     0.     0. ...,     0.     0.     0.]\n [    3.     0.     0. ...,     0.     0.     0.]]\n7.0\n(50L, 50L) (50L, 50L)\n[[  41.   19.    1. ...,    0.    0.    0.]\n [ 110.   71.    1. ...,    0.    0.    0.]\n [   1.   13.    4. ...,    0.    0.    0.]\n ..., \n [   0.    0.    0. ...,    0.    0.    0.]\n [   0.    0.    0. ...,    0.    0.    0.]\n [   0.    0.    0. ...,    0.    0.    0.]]\n9.0\n(50L, 50L) (50L, 50L)\n[[  4.10000000e+01   1.00000000e+01   8.58000000e+02 ...,   0.00000000e+00\n    0.00000000e+00   0.00000000e+00]\n [  1.10000000e+02   5.80000000e+01   6.00000000e+00 ...,   0.00000000e+00\n    0.00000000e+00   0.00000000e+00]\n [  1.00000000e+00   2.09000000e+02   1.46800000e+03 ...,   0.00000000e+00\n    0.00000000e+00   0.00000000e+00]\n ..., \n [  0.00000000e+00   0.00000000e+00   0.00000000e+00 ...,   0.00000000e+00\n    0.00000000e+00   0.00000000e+00]\n [  0.00000000e+00   0.00000000e+00   0.00000000e+00 ...,   0.00000000e+00\n    0.00000000e+00   0.00000000e+00]\n [  0.00000000e+00   0.00000000e+00   0.00000000e+00 ...,   0.00000000e+00\n    0.00000000e+00   0.00000000e+00]]\n8.0\n(50L, 50L) (50L, 50L)\n[[  41.   44.   19. ...,    0.    0.    0.]\n [ 149.   17.   59. ...,    0.    0.    0.]\n [   9.   24.   42. ...,    0.    0.    0.]\n ..., \n [   0.    0.    0. ...,    0.    0.    0.]\n [   0.    0.    0. ...,    0.    0.    0.]\n [   0.    0.    0. ...,    0.    0.    0.]]\n11.0\n(50L, 50L) (50L, 50L)\n[[  40.    9.  148. ...,    0.    0.    0.]\n [ 269.   76.  102. ...,    0.    0.    0.]\n [  11.  120.   90. ...,    0.    0.    0.]\n ..., \n [   0.    0.    0. ...,    0.    0.    0.]\n [   0.    0.    0. ...,    0.    0.    0.]\n [   0.    0.    0. ...,    0.    0.    0.]]\n6.0\n(50L, 50L) (50L, 50L)\n[[  41.   13.    6. ...,    0.    0.    0.]\n [ 110.   17.   47. ...,    0.    0.    0.]\n [   1.    9.  700. ...,    0.    0.    0.]\n ..., \n [   0.    0.    0. ...,    0.    0.    0.]\n [   0.    0.    0. ...,    0.    0.    0.]\n [   0.    0.    0. ...,    0.    0.    0.]]\n10.0\n(50L, 50L) (50L, 50L)\n[[  6.00000000e+00   1.14000000e+02   3.01100000e+03 ...,   0.00000000e+00\n    0.00000000e+00   0.00000000e+00]\n [  1.60000000e+01   1.80000000e+01   2.05700000e+03 ...,   0.00000000e+00\n    0.00000000e+00   0.00000000e+00]\n [  6.60000000e+01   6.00000000e+00   5.00000000e+00 ...,   0.00000000e+00\n    0.00000000e+00   0.00000000e+00]\n ..., \n [  1.68000000e+02   0.00000000e+00   2.50000000e+01 ...,   0.00000000e+00\n    0.00000000e+00   0.00000000e+00]\n [  3.20000000e+01   0.00000000e+00   2.05700000e+03 ...,   0.00000000e+00\n    0.00000000e+00   0.00000000e+00]\n [  3.00000000e+00   0.00000000e+00   3.00000000e+00 ...,   0.00000000e+00\n    0.00000000e+00   0.00000000e+00]]\n8.0\n(50L, 50L) (50L, 50L)\n[[  41.   40.  460. ...,    0.    0.    0.]\n [  81.  269.   18. ...,    0.    0.    0.]\n [  12.   17.  361. ...,    0.    0.    0.]\n ..., \n [   0.    0.    0. ...,    0.    0.    0.]\n [   0.    0.    0. ...,    0.    0.    0.]\n [   0.    0.    0. ...,    0.    0.    0.]]\n7.0\n"
     ]
    }
   ],
   "source": [
    "for i in xrange(20):\n",
    "    X, mask, y = train_batch.next()\n",
    "    print mask.shape,X.shape\n",
    "    print X\n",
    "    print y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'TensorVariable' object does not support item assignment",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-3fb092b8bb5c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mswitch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'TensorVariable' object does not support item assignment"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "import theano.tensor as T\n",
    "import theano\n",
    "import numpy as np\n",
    "\n",
    "from utils import init_weights, init_bias\n",
    "b = np.array([[0, 2], [3, 4]]).astype(\"float32\")\n",
    "a = np.array([1]).astype('float32')\n",
    "m= T.fmatrices(\"m\")\n",
    "x= T.tile(m,(1,3))\n",
    "y=T.switch(x.sum(axis=0)>0,1.0,0.0)\n",
    "y=y[None,:]\n",
    "y[...]=b\n",
    "out = theano.function([m],y)\n",
    "out(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.00509187 -0.06839405 -0.00483718  0.09116028  0.02922797  0.08932855\n   0.16974536  0.26184026  0.16582535  0.30151564]\n [-0.00509187 -0.06839405 -0.00483718  0.09116028  0.02922797  0.08932855\n   0.16974536  0.26184026  0.16582535  0.30151564]]\n(2L, 10L)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x=np.random.randn((10)) * 0.1\n",
    "x=np.tile(x,(1,2))\n",
    "x=x.reshape((2,10))\n",
    "\n",
    "print x\n",
    "print x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100L, 50L)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import pprint\n",
    "import numpy as np\n",
    "from keras.preprocessing import sequence\n",
    "def get_sents(string, vocab, doc_len=100, sent_len=50):\n",
    "    \"\"\"\n",
    "    :param string:\n",
    "    :return: ndarray:shape=(doc_len, sent_len), mask: shape=(doc_len, sent_len), num_hit, unk_hit, total\n",
    "    \"\"\"\n",
    "\n",
    "    num_hit = 0\n",
    "    unk_hit = 0\n",
    "    total = 0\n",
    "    sents = nltk.sent_tokenize(string)\n",
    "    doc_matrix = np.zeros(shape=(doc_len, sent_len))\n",
    "    for i, sent in enumerate(sents):\n",
    "        tokens = nltk.word_tokenize(sent)\n",
    "        indices = []\n",
    "        if i>= doc_len:\n",
    "            break\n",
    "        for word in tokens:\n",
    "            indices.append(1)\n",
    "            total += 1    \n",
    "        doc_matrix[i,:] = sequence.pad_sequences([indices], sent_len, padding='post')\n",
    "    return doc_matrix, num_hit, unk_hit, total\n",
    "x=\"@ORGANIZATION1, @NUM1 out of @NUM2 people surveyed in @LOCATION1, @CAPS1 agree that the use of computers benefits society. I personally concer\" \\\n",
    "             \"n with these @NUM1 people, computers have a positive effect on people.\"\n",
    "# sents = nltk.sent_tokenize(x)\n",
    "print get_sents(x,{'out':1, 'people':2})[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 3, 4], [1, 2, 3]]\n"
     ]
    }
   ],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}